{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8615b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import uuid\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6207ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "with open('../data/summarized_texts.json', 'r' , encoding=\"utf-8\") as f2:\n",
    "    texts = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a6e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You emulate a player of the Stardew Valley game.\n",
    "Here is the text from a wiki page of this game, along with the page and the section it was extracted from.\n",
    "Formulate a question that can be answered using these text materials.\n",
    "Only return the question. The questions should be complete and concise.\n",
    "Page title: {page_title}\n",
    "Section title: {section_title}\n",
    "Text: {text}\\n\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f985544",
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIclient = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-5-nano'):\n",
    "    response = OpenAIclient.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_generation(knowledge_base , sampleNum = 10):\n",
    "\n",
    "    evaluation_questions = []\n",
    "\n",
    "    sample_kb= random.sample(knowledge_base, sampleNum)\n",
    "\n",
    "    for kb in sample_kb:\n",
    "        eval = kb\n",
    "        prompt = question_generation_prompt.format(page_title=kb[\"page_title\"], section_title=kb[\"section_title\"],text=kb[\"text\"]).strip()\n",
    "        question = llm(prompt)\n",
    "        eval[\"question\"] = question\n",
    "        evaluation_questions.append(eval)\n",
    "\n",
    "    return evaluation_questions   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr_and_hitrate(results, k=5):\n",
    "    \"\"\"\n",
    "    results: list of lists of tuples (ranked_docs, correct_doc_id)\n",
    "             e.g. [ ([\"(page1,sec1)\", \"(page2,sec2)\", ...], \"(page2,sec2)\") , ... ]\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    hits = 0\n",
    "\n",
    "    for ranked_docs, correct_doc in results:\n",
    "        # Find rank (1-indexed)\n",
    "        rank = None\n",
    "        for i, doc in enumerate(ranked_docs[:k]):\n",
    "            if doc == correct_doc:\n",
    "                rank = i + 1\n",
    "                break\n",
    "\n",
    "        if rank:\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "            hits += 1\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "\n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    hit_rate = hits / len(results)\n",
    "\n",
    "    return mrr, hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_function(search_function, knowledge_base, k=5, sampleNum=5):\n",
    "\n",
    "    evaluation_dataset = question_generation(knowledge_base, sampleNum)\n",
    "    results = []\n",
    "\n",
    "    for dp in evaluation_dataset:\n",
    "        search_results = search_function(query = dp[\"question\"])\n",
    "        correct_doc = (dp[\"page_title\"], dp[\"section_title\"])\n",
    "\n",
    "        retrieved_ids = [\n",
    "            (doc.payload[\"page_title\"], doc.payload[\"section_title\"])\n",
    "            for doc in search_results\n",
    "        ]\n",
    "\n",
    "        results.append((retrieved_ids, correct_doc))\n",
    "\n",
    "    mrr, hit_rate = compute_mrr_and_hitrate(results, k)\n",
    "    print(f\"MRR@{k}: {mrr:.3f}\")\n",
    "    print(f\"HitRate@{k}: {hit_rate:.3f}\")\n",
    "\n",
    "    return {\"MRR\": mrr, \"HitRate\": hit_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_functions(search_functions, knowledge_base, k=5, sampleNum=5):\n",
    "    \"\"\"\n",
    "    Evaluate multiple search functions on the same evaluation dataset.\n",
    "\n",
    "    Args:\n",
    "        search_functions (list): list of functions or (name, function) tuples.\n",
    "        knowledge_base (list): list of documents with page_title, section_title, text.\n",
    "        k (int): top-k results to consider.\n",
    "        sampleNum (int): number of documents to sample for evaluation.\n",
    "    \"\"\"\n",
    "    evaluation_dataset = question_generation(knowledge_base, sampleNum)\n",
    "    all_results = {}\n",
    "\n",
    "    for item in search_functions:\n",
    "        # Handle both function and (name, function) tuple\n",
    "        if isinstance(item, tuple):\n",
    "            name, search_function = item\n",
    "        else:\n",
    "            search_function = item\n",
    "            name = item.__name__\n",
    "\n",
    "        print(f\"\\nEvaluating: {name}\")\n",
    "        results = []\n",
    "\n",
    "        for dp in evaluation_dataset:\n",
    "            query = dp[\"question\"]\n",
    "            correct_doc = (dp[\"page_title\"], dp[\"section_title\"])\n",
    "\n",
    "            search_results = search_function(query=query)\n",
    "\n",
    "            retrieved_ids = [\n",
    "                (doc.payload[\"page_title\"], doc.payload[\"section_title\"])\n",
    "                for doc in search_results\n",
    "            ]\n",
    "\n",
    "            results.append((retrieved_ids, correct_doc))\n",
    "\n",
    "        mrr, hit_rate = compute_mrr_and_hitrate(results, k)\n",
    "        print(f\"{name} → MRR@{k}: {mrr:.3f}, HitRate@{k}: {hit_rate:.3f}\")\n",
    "\n",
    "        all_results[name] = {\"MRR\": mrr, \"HitRate\": hit_rate}\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1b156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erfan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from RAG_pipeline import multi_stage_search, rrf_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judging the llms\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from llm_judge import LLMJudge\n",
    "\n",
    "# Initialize\n",
    "judge = LLMJudge(OpenAIclient, model='gpt-4o-mini')\n",
    "\n",
    "# Evaluate a search function\n",
    "results = judge.evaluate_retrieval_with_judge(\n",
    "    search_function=multi_stage_search,\n",
    "    knowledge_base=texts,\n",
    "    sample_num=5\n",
    ")\n",
    "\n",
    "print(f\"Average Scores: {results['average_scores']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab6d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5ab8559",
   "metadata": {},
   "source": [
    "# LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b9073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Best answer: gpt-5-mini.  \n",
      "Why: It’s concise, accurate, and adds a useful gameplay detail (how many ore to smelt an Iridium Bar). It covers the main sources and the Skull Cavern depth tip without unnecessary repetition.\n",
      "\n",
      "2) Brief scores (1-10):\n",
      "- gpt-4o: 8 — Accurate and well-structured; slightly more verbose but complete.\n",
      "- gpt-4o-mini: 7 — Similar content to gpt-4o but less polished/concise.\n",
      "- gpt-5-mini: 9 — Concise, accurate, and includes the practical smelting note.\n",
      "\n",
      "3) Notable differences:\n",
      "- gpt-5-mini is the most concise and provides the extra smelting recipe (5 Iridium Ore + 1 Coal → Iridium Bar).\n",
      "- gpt-4o and gpt-4o-mini are very similar to each other; gpt-4o is more polished in formatting.\n",
      "- All three list the same primary sources (Iridium Nodes in Skull Cavern/Volcano Dungeon, geodes, monster drops, fishing chests, panning, fish pond, Statue of Perfection).\n"
     ]
    }
   ],
   "source": [
    "from llm_eval import llm_eval\n",
    "\n",
    "# Test different models\n",
    "models = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-5-mini\"]\n",
    "query = \"How do I get iridium ore in Stardew Valley?\"\n",
    "\n",
    "result = llm_eval(models, query)\n",
    "print(result[\"evaluation\"])  # Judge's evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1d11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
